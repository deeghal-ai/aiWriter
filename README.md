# BikeDekho AI Writer

An AI-powered motorcycle comparison article generator with a centralized, optimized architecture.

## Tech Stack

- **Frontend Framework:** Next.js 14 (App Router)
- **Styling:** Tailwind CSS
- **UI Components:** shadcn/ui
- **Icons:** Lucide React
- **State Management:** Zustand
- **AI Provider:** Anthropic Claude (Haiku 3.5, Sonnet 4, Opus 4)
- **TypeScript:** Full type safety

---

## Architecture Overview

### Core Design Principles

1. **Centralized Configuration** - All AI models and configs managed in single registry
2. **Factory Pattern** - Simple routes use factory for provider abstraction
3. **Direct + Helpers** - Complex multi-stage routes use registry helpers for transparency
4. **Optimized Prompts** - Single prompt system (optimized, terse, XML-based)
5. **Type Safety** - End-to-end TypeScript with strict typing

---

## Data Flow Architecture

### ğŸ”„ Complete Pipeline

```
User Input (Step 1)
    â†“
Forum Scraping (Step 2)
    â”œâ”€ Reddit API
    â”œâ”€ YouTube API
    â””â”€ xBhp Scraping
    â†“
Insight Extraction (Step 3) â† Factory Pattern
    â†“
Persona Generation (Step 4) â† Factory Pattern
    â†“
Verdict Generation (Step 5) â† Factory Pattern
    â†“
Article Generation (Step 6) â† Direct + Registry Helpers
    â”œâ”€ Planning (1 AI call)
    â”œâ”€ Sections (7+ AI calls)
    â””â”€ Coherence (1 AI call)
    â†“
Final Review (Step 8)
```

---

## ğŸ—ï¸ Detailed Architecture by Stage

### 1. Insight Extraction (Step 3)

**Pattern:** Factory Pattern  
**Flow:** API Route â†’ Factory â†’ Provider â†’ Model Registry

#### Files Involved
```
src/app/api/extract/insights/route.ts
    â†“ calls
src/lib/ai/factory.ts â†’ extractInsightsOptimized()
    â†“ delegates to
src/lib/ai/providers/claude.ts â†’ extractInsightsOptimized()
    â†“ gets config from
src/lib/ai/models/registry.ts â†’ getModelApiConfig('extraction')
    â†“ builds prompts with
src/lib/ai/prompts-optimized.ts â†’ buildSingleBikeExtractionPrompt()
    â†“ uses system prompt
src/lib/ai/prompts-optimized.ts â†’ EXTRACTION_SYSTEM_PROMPT
```

#### Actual Function Flow
```typescript
// 1. API Route receives request
POST /api/extract/insights
  â”œâ”€ validates input
  â”œâ”€ combines data sources
  â””â”€ calls extractInsightsOptimized(bike1Name, bike2Name, combinedData)

// 2. Factory delegates to provider
extractInsightsOptimized() in factory.ts
  â”œâ”€ gets AI provider
  â””â”€ calls provider.extractInsightsOptimized()

// 3. Provider processes with parallel extraction
ClaudeProvider.extractInsightsOptimized()
  â”œâ”€ gets model config: getModelApiConfig('extraction')
  â”‚   â””â”€ returns { model: 'claude-3-5-haiku-20241022', temp: 0.1, maxTokens: 4096 }
  â”œâ”€ prepares data for both bikes
  â”œâ”€ calls extractSingleBikeOptimized() for bike1 (parallel)
  â”œâ”€ calls extractSingleBikeOptimized() for bike2 (parallel)
  â””â”€ combines results

// 4. Each bike extraction
extractSingleBikeOptimized(bikeName, bikeData, modelConfig)
  â”œâ”€ builds prompt: buildSingleBikeExtractionPrompt(bikeName, bikeData)
  â”œâ”€ uses system prompt: EXTRACTION_SYSTEM_PROMPT
  â”œâ”€ calls Claude API with modelConfig
  â”œâ”€ parses JSON response
  â””â”€ returns BikeInsights { praises, complaints, surprising_insights }
```

#### Model Configuration
```typescript
// From registry.ts
extraction: {
  modelId: 'claude-haiku-3.5',    // Fast, cheap model
  temperature: 0.1,                 // Low for factual extraction
  maxTokens: 4096,
  description: 'Extract insights from forum discussions'
}
```

#### Response Structure
```typescript
{
  bike1: {
    name: "Royal Enfield Classic 350",
    praises: [
      {
        category: "Highway stability at 100kmph",
        frequency: 12,
        quotes: [{ text: "...", author: "...", source: "YouTube|Reddit" }]
      }
    ],
    complaints: [...],
    surprising_insights: ["..."]
  },
  bike2: { ... },
  metadata: { extracted_at, total_praises, total_complaints, total_quotes }
}
```

---

### 2. Persona Generation (Step 4)

**Pattern:** Factory Pattern  
**Flow:** API Route â†’ Factory â†’ Provider â†’ Model Registry

#### Files Involved
```
src/app/api/generate/personas/route.ts
    â†“ calls
src/lib/ai/factory.ts â†’ generatePersonasOptimized()
    â†“ delegates to
src/lib/ai/providers/claude.ts â†’ generatePersonasOptimized()
    â†“ gets config from
src/lib/ai/models/registry.ts â†’ getModelApiConfig('personas')
    â†“ builds prompts with
src/lib/ai/prompts-optimized.ts â†’ buildOptimizedPersonaPrompt()
    â†“ uses system prompt
src/lib/ai/prompts-optimized.ts â†’ PERSONA_SYSTEM_PROMPT
```

#### Actual Function Flow
```typescript
// 1. API Route receives request
POST /api/generate/personas
  â”œâ”€ validates insights exist
  â””â”€ calls generatePersonasOptimized(bike1Name, bike2Name, insights)

// 2. Factory delegates to provider
generatePersonasOptimized() in factory.ts
  â”œâ”€ gets AI provider
  â””â”€ calls provider.generatePersonasOptimized()

// 3. Provider processes
ClaudeProvider.generatePersonasOptimized()
  â”œâ”€ gets model config: getModelApiConfig('personas')
  â”‚   â””â”€ returns { model: 'claude-3-5-haiku-20241022', temp: 0.3, maxTokens: 6144 }
  â”œâ”€ builds prompt: buildOptimizedPersonaPrompt(bike1Name, bike2Name, insights)
  â”‚   â””â”€ condenses insights (top 5 praises, top 4 complaints)
  â”‚   â””â”€ includes few-shot examples
  â”‚   â””â”€ uses XML tags for structure
  â”œâ”€ uses system prompt: PERSONA_SYSTEM_PROMPT
  â”œâ”€ calls Claude API
  â”œâ”€ parses JSON response
  â”œâ”€ validates persona structure
  â””â”€ returns PersonaGenerationResult

// 4. Prompt optimization
buildOptimizedPersonaPrompt()
  â”œâ”€ calls condenseInsightsForPersonas(insights)
  â”‚   â””â”€ reduces data: top praises/complaints per bike
  â”‚   â””â”€ 30-40% token reduction
  â”œâ”€ builds XML-structured prompt
  â”œâ”€ includes golden rules and anti-patterns
  â””â”€ provides example persona
```

#### Model Configuration
```typescript
// From registry.ts
personas: {
  modelId: 'claude-haiku-3.5',
  temperature: 0.3,        // Moderate for creativity
  maxTokens: 6144,        // More for detailed personas
  description: 'Generate detailed rider personas'
}
```

#### Response Structure
```typescript
{
  personas: [
    {
      id: "persona-1",
      name: "Arjun - The Silk Board Survivor",
      title: "IT professional with 70% city commute",
      percentage: 28,
      sampleSize: 12,
      usagePattern: { cityCommute: 70, highway: 20, urbanLeisure: 10, offroad: 0 },
      demographics: { ageRange: "28-35", cityType: "metro", ... },
      psychographics: { buyingMotivation: "...", decisionStyle: "...", ... },
      priorities: ["Reliability above all", "Comfortable for daily commute", ...],
      painPoints: ["Current bike's seat destroys lower back", ...],
      evidenceQuotes: ["...", "..."],
      archetypeQuote: "I need a bike that won't leave me stranded",
      color: "blue"
    }
  ],
  metadata: { generated_at, total_personas, total_evidence_quotes }
}
```

---

### 3. Verdict Generation (Step 5)

**Pattern:** Factory Pattern (with Parallel Processing)  
**Flow:** API Route â†’ Factory â†’ Provider â†’ Parallel AI Calls

#### Files Involved
```
src/app/api/generate/verdicts/route.ts
    â†“ calls
src/lib/ai/factory.ts â†’ generateVerdictsOptimized()
    â†“ delegates to
src/lib/ai/providers/claude.ts â†’ generateVerdictsOptimized()
    â†“ gets config from
src/lib/ai/models/registry.ts â†’ getModelApiConfig('verdicts')
    â†“ builds prompts with
src/lib/ai/prompts-optimized.ts â†’ buildSingleVerdictPrompt()
    â†“ uses system prompt
src/lib/ai/prompts-optimized.ts â†’ VERDICT_SYSTEM_PROMPT
```

#### Actual Function Flow
```typescript
// 1. API Route receives request
POST /api/generate/verdicts
  â”œâ”€ validates personas and insights exist
  â””â”€ calls generateVerdictsOptimized(bike1Name, bike2Name, personas, insights)

// 2. Factory delegates to provider
generateVerdictsOptimized() in factory.ts
  â”œâ”€ gets AI provider
  â””â”€ calls provider.generateVerdictsOptimized()

// 3. Provider processes in PARALLEL (3-5x faster)
ClaudeProvider.generateVerdictsOptimized()
  â”œâ”€ gets model config: getModelApiConfig('verdicts')
  â”‚   â””â”€ returns { model: 'claude-3-5-haiku-20241022', temp: 0.2, maxTokens: 2048 }
  â”œâ”€ creates parallel promises for each persona
  â”‚   â””â”€ Promise.all(personas.map(p => generateSingleVerdictOptimized(p)))
  â””â”€ waits for all verdicts to complete

// 4. Each verdict generation (runs in parallel)
generateSingleVerdictOptimized(bike1Name, bike2Name, persona, insights)
  â”œâ”€ builds persona-specific prompt: buildSingleVerdictPrompt()
  â”‚   â””â”€ filters insights relevant to persona's priorities
  â”‚   â””â”€ includes persona's usage pattern and pain points
  â”œâ”€ uses system prompt: VERDICT_SYSTEM_PROMPT
  â”œâ”€ calls Claude API
  â”œâ”€ parses JSON verdict
  â”œâ”€ normalizes bike names (handles variations)
  â””â”€ returns Verdict

// 5. Results compilation
  â”œâ”€ calculates bike1Wins vs bike2Wins
  â”œâ”€ computes average confidence
  â”œâ”€ identifies closest call
  â””â”€ returns VerdictGenerationResult
```

#### Model Configuration
```typescript
// From registry.ts
verdicts: {
  modelId: 'claude-haiku-3.5',
  temperature: 0.2,        // Low-moderate for reasoning
  maxTokens: 2048,        // Moderate for verdict reasoning
  description: 'Generate bike recommendations per persona'
}
```

#### Parallel Processing Advantage
```
Traditional (Sequential):
Persona 1 (4s) â†’ Persona 2 (4s) â†’ Persona 3 (4s) = 12 seconds

Optimized (Parallel):
Persona 1 (4s) â”
Persona 2 (4s) â”œâ”€â†’ Max = 4 seconds (3x faster!)
Persona 3 (4s) â”˜
```

#### Response Structure
```typescript
{
  verdicts: [
    {
      personaId: "persona-1",
      personaName: "Arjun",
      personaTitle: "The Silk Board Survivor",
      recommendedBike: "Honda CB350",
      otherBike: "Royal Enfield Classic 350",
      confidence: 78,
      confidenceExplanation: "Clear winner on 2 of 3 priorities",
      reasoning: [
        {
          point: "Superior reliability track record",
          priority: "Reliability above all",
          evidence: "Zero major complaints in 40+ forum posts"
        }
      ],
      againstReasons: ["If resale value is deciding factor, RE holds value 15-20% better"],
      tangibleImpact: { metric: "Service visits/year", value: "2 vs 4", ... },
      verdictOneLiner: "For a reliability-obsessed upgrader, Honda wins"
    }
  ],
  summary: {
    bike1Wins: 2,
    bike2Wins: 1,
    closestCall: "Persona-2 was closest at 62% confidence"
  },
  metadata: { generated_at, total_verdicts, average_confidence }
}
```

---

### 4. Article Generation (Step 6)

**Pattern:** Direct + Registry Helpers (Multi-Stage Pipeline)  
**Flow:** API Route â†’ Direct Anthropic Client â†’ Registry Configs

#### Why Different Pattern?
Article generation is **complex** with 10+ AI calls. Factory pattern would hide orchestration logic. Direct approach with centralized configs provides transparency while maintaining centralization.

#### Files Involved
```
src/app/api/generate/article/route.ts
    â†“ gets configs from
src/lib/ai/models/registry.ts
    â”œâ”€ getModelApiConfig('article_planning')   â†’ Sonnet 4
    â”œâ”€ getModelApiConfig('article_writing')    â†’ Haiku 3.5
    â””â”€ getModelApiConfig('article_coherence')  â†’ Haiku 3.5
    â†“ builds prompts from
src/lib/ai/article-planner.ts â†’ buildNarrativePlanningPrompt()
src/lib/ai/article-sections/hook.ts â†’ buildHookPrompt()
src/lib/ai/article-sections/truth-bomb.ts â†’ buildTruthBombPrompt()
src/lib/ai/article-sections/personas.ts â†’ buildPersonasPrompt()
src/lib/ai/article-sections/matrix.ts â†’ buildMatrixPrompt()
src/lib/ai/article-sections/contrarian.ts â†’ buildContrarianPrompt()
src/lib/ai/article-sections/verdicts.ts â†’ buildVerdictsPrompt()
src/lib/ai/article-sections/bottom-line.ts â†’ buildBottomLinePrompt()
    â†“ coherence check
src/lib/ai/article-coherence.ts â†’ buildCoherencePrompt()
    â†“ quality validation
src/lib/ai/article-quality-check.ts â†’ checkArticleQuality()
```

#### Actual Function Flow
```typescript
// 1. API Route receives request
POST /api/generate/article
  â”œâ”€ validates insights, personas, verdicts exist
  â”œâ”€ creates Anthropic client
  â””â”€ orchestrates 3 phases

// PHASE 1: Narrative Planning (1 AI call)
generateNarrativePlan()
  â”œâ”€ gets config: getModelApiConfig('article_planning')
  â”‚   â””â”€ { model: 'claude-sonnet-4-20250514', temp: 0.5, maxTokens: 4096 }
  â”œâ”€ builds prompt: buildNarrativePlanningPrompt(bikes, insights, personas, verdicts)
  â”œâ”€ calls Claude API (Sonnet for strategic planning)
  â””â”€ returns NarrativePlan {
        story_angle,
        hook_strategy,
        matrix_focus_areas: ["Engine Character", "Comfort", "Value"],
        contrarian_angle,
        quote_allocation,
        callbacks
      }

// PHASE 2: Section Generation (7+ AI calls)
// Parallel batch 1 (3 independent sections)
Promise.all([
  generateSection('hook'),           // Uses Haiku
  generateSection('truthBomb'),      // Uses Haiku
  generateSection('personas')        // Uses Haiku
])

// Parallel batch 2 (matrix sections - typically 3)
Promise.all(
  matrix_focus_areas.map(area => 
    generateMatrixSection(area)      // Uses Haiku for each
  )
)

// Sequential (depend on narrative)
generateSection('contrarian')        // Uses Haiku
generateSection('verdicts')          // Uses Haiku
generateSection('bottomLine')        // Uses Haiku

// Each generateSection() call:
generateSection(client, sectionType, data)
  â”œâ”€ gets config: getModelApiConfig('article_writing')
  â”‚   â””â”€ { model: 'claude-3-5-haiku-20241022', temp: 0.7, maxTokens: 4096 }
  â”œâ”€ selects prompt builder based on sectionType
  â”‚   â”œâ”€ 'hook' â†’ buildHookPrompt(bike1Name, bike2Name, narrativePlan, insights)
  â”‚   â”œâ”€ 'truthBomb' â†’ buildTruthBombPrompt(narrativePlan, insights)
  â”‚   â”œâ”€ 'personas' â†’ buildPersonasPrompt(personas, narrativePlan)
  â”‚   â”œâ”€ 'contrarian' â†’ buildContrarianPrompt(winner, loser, narrativePlan, verdicts)
  â”‚   â”œâ”€ 'verdicts' â†’ buildVerdictsPrompt(verdicts, personas, narrativePlan)
  â”‚   â””â”€ 'bottomLine' â†’ buildBottomLinePrompt(bikes, narrativePlan, verdicts)
  â”œâ”€ calls Claude API with config
  â””â”€ returns section text

// PHASE 3: Coherence Pass (1 AI call)
runCoherencePass(client, sections, narrativePlan)
  â”œâ”€ gets config: getModelApiConfig('article_coherence')
  â”‚   â””â”€ { model: 'claude-3-5-haiku-20241022', temp: 0.3, maxTokens: 2048 }
  â”œâ”€ builds prompt: buildCoherencePrompt(sections, narrativePlan)
  â”œâ”€ calls Claude API
  â””â”€ returns CoherenceEdits { transitions, callbacks, tone_adjustments }

// Apply edits and quality check
applyCoherenceEdits(sections, coherenceEdits)
checkArticleQuality(sections, bike1Name, bike2Name, personas)
```

#### Model Configurations
```typescript
// From registry.ts
article_planning: {
  modelId: 'claude-sonnet-4',      // Smarter model for strategy
  temperature: 0.5,                 // Creative planning
  maxTokens: 4096,
  description: 'Plan article narrative structure'
},
article_writing: {
  modelId: 'claude-haiku-3.5',     // Fast model for writing
  temperature: 0.7,                 // Creative writing
  maxTokens: 4096,
  description: 'Write creative article sections'
},
article_coherence: {
  modelId: 'claude-haiku-3.5',
  temperature: 0.3,                 // Focused editing
  maxTokens: 2048,
  description: 'Check article coherence and flow'
}
```

#### AI Call Breakdown
```
Planning:      1 call  (Sonnet - strategic)
Sections:      7+ calls (Haiku - fast writing)
  â”œâ”€ Hook:     1 call
  â”œâ”€ Truth:    1 call
  â”œâ”€ Personas: 1 call
  â”œâ”€ Matrix:   3+ calls (one per focus area)
  â”œâ”€ Contrarian: 1 call
  â”œâ”€ Verdicts: 1 call
  â””â”€ Bottom:   1 call
Coherence:     1 call  (Haiku - editing)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:         ~10 calls per article
```

#### Response Structure
```typescript
{
  sections: [
    {
      id: 'hook',
      title: 'The Hook',
      content: '...',        // Generated text
      wordCount: 250,
      status: 'complete'
    },
    {
      id: 'truth',
      title: 'The Truth',
      content: '...',
      wordCount: 180,
      status: 'complete'
    },
    // ... all sections
  ],
  narrativePlan: { ... },
  qualityReport: {
    overall_score: 85,
    readability: "good",
    issues: [],
    suggestions: []
  },
  metadata: {
    generated_at,
    total_words: 2847,
    section_count: 8,
    processing_time_ms: 45000
  }
}
```

---

## ğŸ“ Project Structure

```
src/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ extract/
â”‚   â”‚   â”‚   â””â”€â”€ insights/
â”‚   â”‚   â”‚       â””â”€â”€ route.ts          # Extraction API (Factory Pattern)
â”‚   â”‚   â”œâ”€â”€ generate/
â”‚   â”‚   â”‚   â”œâ”€â”€ personas/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ route.ts          # Persona API (Factory Pattern)
â”‚   â”‚   â”‚   â”œâ”€â”€ verdicts/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ route.ts          # Verdict API (Factory Pattern)
â”‚   â”‚   â”‚   â””â”€â”€ article/
â”‚   â”‚   â”‚       â”œâ”€â”€ route.ts          # Article API (Direct + Helpers)
â”‚   â”‚   â”‚       â””â”€â”€ streaming/
â”‚   â”‚   â”‚           â””â”€â”€ route.ts      # Streaming Article API
â”‚   â”‚   â””â”€â”€ scrape/
â”‚   â”‚       â”œâ”€â”€ reddit/route.ts       # Reddit scraping
â”‚   â”‚       â”œâ”€â”€ youtube/route.ts      # YouTube scraping
â”‚   â”‚       â””â”€â”€ xbhp/route.ts        # xBhp scraping
â”‚   â”œâ”€â”€ layout.tsx
â”‚   â”œâ”€â”€ page.tsx
â”‚   â””â”€â”€ globals.css
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ui/                          # shadcn components
â”‚   â”œâ”€â”€ layout/
â”‚   â”‚   â”œâ”€â”€ AppHeader.tsx
â”‚   â”‚   â””â”€â”€ StepSidebar.tsx
â”‚   â””â”€â”€ steps/
â”‚       â”œâ”€â”€ Step1Input.tsx           # Bike input
â”‚       â”œâ”€â”€ Step2Scrape.tsx         # Forum scraping
â”‚       â”œâ”€â”€ Step3Extract.tsx        # Insight extraction
â”‚       â”œâ”€â”€ Step4Personas.tsx       # Persona generation
â”‚       â”œâ”€â”€ Step5Verdicts.tsx       # Verdict generation
â”‚       â”œâ”€â”€ Step6Article.tsx        # Article generation
â”‚       â”œâ”€â”€ Step7Polish.tsx         # Quality checks
â”‚       â””â”€â”€ Step8Review.tsx         # Final review
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â””â”€â”€ registry.ts         # â­ CENTRAL MODEL REGISTRY
â”‚   â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â”‚   â”œâ”€â”€ claude.ts           # Claude implementation
â”‚   â”‚   â”‚   â”œâ”€â”€ base-provider.ts    # Provider interface
â”‚   â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â”‚   â”œâ”€â”€ article-sections/
â”‚   â”‚   â”‚   â”œâ”€â”€ hook.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ truth-bomb.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ personas.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ matrix.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ contrarian.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ verdicts.ts
â”‚   â”‚   â”‚   â””â”€â”€ bottom-line.ts
â”‚   â”‚   â”œâ”€â”€ factory.ts              # â­ FACTORY LAYER
â”‚   â”‚   â”œâ”€â”€ prompts-optimized.ts    # â­ OPTIMIZED PROMPTS (ACTIVE)
â”‚   â”‚   â”œâ”€â”€ prompts.ts             # (Deprecated - legacy)
â”‚   â”‚   â”œâ”€â”€ article-planner.ts
â”‚   â”‚   â”œâ”€â”€ article-coherence.ts
â”‚   â”‚   â””â”€â”€ article-quality-check.ts
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ reddit.ts
â”‚   â”‚   â”œâ”€â”€ youtube.ts
â”‚   â”‚   â”œâ”€â”€ xbhp.ts
â”‚   â”‚   â””â”€â”€ sonnet-data-prep.ts
â”‚   â”œâ”€â”€ store.ts                    # Zustand state management
â”‚   â”œâ”€â”€ types.ts                    # TypeScript types
â”‚   â””â”€â”€ utils.ts
â””â”€â”€ utils/
    â””â”€â”€ validation.ts                # Validation & quality checks
```

### Key Files

| File | Purpose | Pattern |
|------|---------|---------|
| `models/registry.ts` | Central model configuration | Registry |
| `factory.ts` | Provider abstraction layer | Factory |
| `prompts-optimized.ts` | All AI prompts | Optimized prompts |
| `providers/claude.ts` | Claude API implementation | Provider |
| `api/*/route.ts` | API endpoints | Route handlers |

---

## ğŸ”§ Configuration

### Environment Variables

```bash
# .env.local
ANTHROPIC_API_KEY=sk-ant-...          # Required
REDDIT_CLIENT_ID=...                   # For Reddit scraping
REDDIT_CLIENT_SECRET=...               # For Reddit scraping
YOUTUBE_API_KEY=...                    # For YouTube scraping
```

### Model Configuration

All models configured in `src/lib/ai/models/registry.ts`:

```typescript
const DEFAULT_TASK_CONFIG = {
  extraction: {
    modelId: 'claude-haiku-3.5',      // Fast extraction
    temperature: 0.1,
    maxTokens: 4096
  },
  personas: {
    modelId: 'claude-haiku-3.5',      // Creative personas
    temperature: 0.3,
    maxTokens: 6144
  },
  verdicts: {
    modelId: 'claude-haiku-3.5',      // Reasoning
    temperature: 0.2,
    maxTokens: 2048
  },
  article_planning: {
    modelId: 'claude-sonnet-4',       // Strategic planning
    temperature: 0.5,
    maxTokens: 4096
  },
  article_writing: {
    modelId: 'claude-haiku-3.5',      // Fast writing
    temperature: 0.7,
    maxTokens: 4096
  },
  article_coherence: {
    modelId: 'claude-haiku-3.5',      // Focused editing
    temperature: 0.3,
    maxTokens: 2048
  }
};
```

### Available Models

```typescript
// From MODEL_REGISTRY in registry.ts
{
  'claude-haiku-3.5': {
    speed: 'fast',
    quality: 'standard',
    cost: '$0.001 input / $0.005 output per 1K tokens'
  },
  'claude-sonnet-4': {
    speed: 'medium',
    quality: 'high',
    cost: '$0.003 input / $0.015 output per 1K tokens',
    recommended: true
  },
  'claude-opus-4': {
    speed: 'slow',
    quality: 'premium',
    cost: '$0.015 input / $0.075 output per 1K tokens'
  }
}
```

---

## ğŸš€ Getting Started

### 1. Install Dependencies

```bash
npm install
```

### 2. Configure Environment

```bash
cp .env.example .env.local
# Add your ANTHROPIC_API_KEY
```

### 3. Run Development Server

```bash
npm run dev
```

Open [http://localhost:3000](http://localhost:3000)

---

## ğŸ“Š Performance Metrics

### Optimization Results

| Stage | Old Approach | New Approach | Improvement |
|-------|-------------|--------------|-------------|
| Extraction | Sequential (60s) | Parallel (20s) | **3x faster** |
| Personas | Verbose prompts (15s) | Optimized prompts (10s) | **33% faster** |
| Verdicts | Sequential (45s) | Parallel (12s) | **3.75x faster** |
| Article | N/A | Multi-stage (45s) | Baseline |

### Token Savings

| Prompt Type | Standard | Optimized | Savings |
|-------------|----------|-----------|---------|
| Extraction | ~8,000 tokens | ~5,000 tokens | **37%** |
| Personas | ~12,000 tokens | ~7,500 tokens | **37%** |
| Verdicts | ~10,000 tokens | ~3,000 tokens | **70%** |

---

## ğŸ§ª Testing

### Manual Testing Flow

```bash
# 1. Start dev server
npm run dev

# 2. Test each stage
curl -X POST http://localhost:3000/api/extract/insights \
  -H "Content-Type: application/json" \
  -d '{"bike1Name": "...", "bike2Name": "...", "redditData": {...}}'

curl -X POST http://localhost:3000/api/generate/personas \
  -H "Content-Type: application/json" \
  -d '{"bike1Name": "...", "bike2Name": "...", "insights": {...}}'

curl -X POST http://localhost:3000/api/generate/verdicts \
  -H "Content-Type: application/json" \
  -d '{"bike1Name": "...", "bike2Name": "...", "personas": {...}, "insights": {...}}'

curl -X POST http://localhost:3000/api/generate/article \
  -H "Content-Type: application/json" \
  -d '{"bike1Name": "...", "bike2Name": "...", "insights": {...}, "personas": {...}, "verdicts": {...}}'
```

### Testing Strategy

1. **Unit Tests** - Test individual prompt builders
2. **Integration Tests** - Test full API routes
3. **End-to-End Tests** - Test complete pipeline
4. **Performance Tests** - Measure response times

---

## ğŸ“– API Documentation

### POST /api/extract/insights

**Request:**
```typescript
{
  bike1Name: string,
  bike2Name: string,
  redditData?: { bike1: {...}, bike2: {...} },
  youtubeData?: { bike1: {...}, bike2: {...} }
}
```

**Response:**
```typescript
{
  success: true,
  data: InsightExtractionResult,
  meta: { processingTimeMs: number }
}
```

### POST /api/generate/personas

**Request:**
```typescript
{
  bike1Name: string,
  bike2Name: string,
  insights: InsightExtractionResult
}
```

**Response:**
```typescript
{
  success: true,
  data: PersonaGenerationResult
}
```

### POST /api/generate/verdicts

**Request:**
```typescript
{
  bike1Name: string,
  bike2Name: string,
  personas: PersonaGenerationResult,
  insights: InsightExtractionResult
}
```

**Response:**
```typescript
{
  success: true,
  data: VerdictGenerationResult
}
```

### POST /api/generate/article

**Request:**
```typescript
{
  bike1Name: string,
  bike2Name: string,
  insights: InsightExtractionResult,
  personas: PersonaGenerationResult,
  verdicts: VerdictGenerationResult
}
```

**Response:**
```typescript
{
  success: true,
  data: {
    sections: ArticleSection[],
    narrativePlan: NarrativePlan,
    qualityReport: QualityReport,
    metadata: { total_words, section_count, processing_time_ms }
  }
}
```

---

## ğŸ”„ Adding New Models

### 1. Add to Registry

```typescript
// src/lib/ai/models/registry.ts
export const MODEL_REGISTRY: ModelDefinition[] = [
  // ... existing models
  {
    id: 'new-model',
    provider: 'anthropic',
    name: 'New Model',
    modelString: 'claude-...',
    capabilities: ['extraction', 'synthesis'],
    speed: 'fast',
    quality: 'standard',
    costPer1kTokens: { input: 0.001, output: 0.005 },
    maxTokens: 4096,
    contextWindow: 200000,
    description: 'Description',
    enabled: true
  }
];
```

### 2. Update Task Config (Optional)

```typescript
// To use new model for a specific task
const DEFAULT_TASK_CONFIG = {
  extraction: {
    modelId: 'new-model',  // Changed!
    temperature: 0.1,
    maxTokens: 4096
  }
};
```

### 3. That's It!

All routes automatically use the new model. No code changes needed.

---

## ğŸ› ï¸ Troubleshooting

### Common Issues

**1. "Anthropic API key not configured"**
- Add `ANTHROPIC_API_KEY=sk-ant-...` to `.env.local`

**2. "Response was truncated"**
- Increase `maxTokens` in registry for that task

**3. "Invalid response structure"**
- Check prompt outputs match expected schema
- Review validation in `utils/validation.ts`

**4. Slow performance**
- Use faster models (Haiku instead of Sonnet)
- Reduce `maxTokens` if possible
- Enable parallel processing where available

---

## ğŸ“ Scripts

```bash
# Development
npm run dev              # Start dev server

# Build
npm run build           # Production build
npm start              # Start production server

# Code Quality
npm run lint           # Run ESLint
npm run type-check     # Run TypeScript checks

# Deployment
npm run deploy         # Deploy to Vercel
```

---

## ğŸ›ï¸ Architecture Decisions

### Why Factory Pattern for Extract/Personas/Verdicts?

**Benefits:**
- âœ… Clean abstraction (routes don't know about AI providers)
- âœ… Easy to swap providers (OpenAI, Google, etc.)
- âœ… Centralized error handling
- âœ… Consistent logging

### Why Direct + Helpers for Article Generation?

**Benefits:**
- âœ… Transparent orchestration (10+ AI calls visible)
- âœ… Easy to debug multi-stage pipeline
- âœ… Flexible section ordering
- âœ… Still centralized via registry helpers

### Why Optimized Prompts Only?

**Benefits:**
- âœ… 30-70% token savings
- âœ… Faster AI responses
- âœ… Lower costs
- âœ… XML structure improves parsing
- âœ… Few-shot examples improve quality

---

## ğŸ¤ Contributing

1. Follow existing patterns (Factory for simple, Direct+Helpers for complex)
2. Use registry for ALL model configs
3. Use optimized prompts
4. Add validation for new endpoints
5. Update this README for architectural changes

---

## ğŸ“„ License

MIT

---

## ğŸ™ Acknowledgments

- Built with Next.js 14 and Anthropic Claude
- UI components from shadcn/ui
- Icons from Lucide React

---

**Last Updated:** December 1, 2025  
**Version:** 2.0.0 (Post-Centralization Refactoring)
